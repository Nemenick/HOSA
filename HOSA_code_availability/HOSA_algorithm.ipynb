{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Trace Stage (STS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library & read Data\n",
    "import _Library_HOSA\n",
    "import time\n",
    "import numpy as np\n",
    "from _Library_HOSA import ClasseDataset\n",
    "import pandas as pd\n",
    "import gc\n",
    "from obspy import UTCDateTime \n",
    "\n",
    "# paths of the dataset\n",
    "hd = \"/home/silvia/Desktop/Data/DETECT/Detect_all_data_extended.hdf5\"\n",
    "cs = \"/home/silvia/Desktop/Data/DETECT/Detect_all_metadata_extended.csv\"\n",
    "# Dataset containing the traces (D.seismogram) and other metadata (D.metadata)\n",
    "D = ClasseDataset()\n",
    "D.leggi_custom_dataset(hd,cs)                                       \n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Start STS\")\n",
    "\n",
    "# uu used to generate the output in a CSV file, save the onset for each trace \n",
    "uu = pd.DataFrame.from_dict(D.metadata[\"trace_name\"])              \n",
    "uu[\"trace_P_arrival_sample\"] = D.metadata[\"trace_P_arrival_sample\"]\n",
    "\n",
    "# p contains the various settings to be tested [statistic, filtertype, frequencies of filtering, window size of HOS, thersholds]\n",
    "p = [[_Library_HOSA.S_4, \"bandpass\", [1,30], 300, [0.1,0.2,0.3,0.4]], \n",
    "     [_Library_HOSA.S_6, \"bandpass\", [1,30], 300, [0.1,0.2,0.3,0.4]]]\n",
    "post_origin = 8\n",
    "\n",
    "# used to generate a string that report the used statistic\n",
    "names = [\"S_4\", \"S_n\"]                                                     \n",
    "indi = 0\n",
    "Tempo_Inizio = time.perf_counter()\n",
    "\n",
    "# cycle on settings\n",
    "for stat, filt, freq, wind, th in p:                                \n",
    "    for ii in range(10):\n",
    "         # free some memory, if needed\n",
    "        gc.collect()  \n",
    "\n",
    "    # key of the final dictionary\n",
    "    string_key = f\"stat: {str(names[indi])} type_filter: {filt} filter freq: {freq} window_width: {wind} tresh:\"    \n",
    "    print(\"I am working on impostation: \",hd,\"\\n\", string_key)\n",
    "    # I use different threshold for same setting\n",
    "    ons_th = [[] for i in range(len(th)) ]                             \n",
    "    ons_max = []\n",
    "    \n",
    "    for i in range(len(D.sismogramma)): \n",
    "        # register the sample corresponding to the event origin\n",
    "        or_s =  int((UTCDateTime(D.metadata[\"source_origin_time\"][i])- UTCDateTime(D.metadata[\"trace_start_time\"][i]))*D.metadata[\"sampling_rate\"][i])\n",
    "        try:\n",
    "            inizio = or_s-wind if or_s-wind > 0  else 0\n",
    "            fine = inizio + post_origin*int(D.metadata[\"sampling_rate\"][i])\n",
    "            # I extract the portion of the waveform from the arrival until post_origin seconds after\n",
    "            sig = _Library_HOSA.freq_filter(D.sismogramma[i][inizio:fine], D.metadata[\"sampling_rate\"][i], freq, type_filter= filt)\n",
    "            # returns onsets for various th_s, CF (HOS derivative), onset corresponding to th=1, lower_bound, HOS\n",
    "            onset_th, diff, onset_max,lower_bound,hoss  = _Library_HOSA.get_onset_4(sig, wind, threshold=th, statistics= stat) \n",
    "            for j in range(len(th)):\n",
    "                ons_th[j].append(onset_th[j] + or_s-wind)\n",
    "            ons_max.append(onset_max + or_s-wind)\n",
    "        except Exception as e: \n",
    "            print(e,\"\\nException occurred to trace\", i)\n",
    "            for j in range(len(th)):\n",
    "                ons_th[j].append(9*10**10)\n",
    "            ons_max.append(9*10**10)\n",
    "\n",
    "\n",
    "    for j in range(len(th)):\n",
    "        ons_th_tmp = np.array(ons_th[j])\n",
    "        uu = pd.concat([uu,pd.DataFrame.from_dict({f\"{string_key}_ons_th={th[j]}\":ons_th_tmp})],axis=1)\n",
    "\n",
    "    ons_max = np.array(ons_max)\n",
    "    uu = pd.concat([uu,pd.DataFrame.from_dict({f\"{string_key}_ons_max\":ons_max})],axis=1)\n",
    "    uu.to_csv(\"/home/silvia/Desktop/ONSET_HOS/ONSET_DETECT_whole.csv\",index=False)\n",
    "    indi +=1\n",
    "print(\"\\nAlgoritmh ran for \", time.perf_counter()-Tempo_Inizio, \" to perform \", len(p), \"tests on \", len(D.sismogramma),\" traces each test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Channel Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum allowed distance for clustering for each array (dmaxs = )\n",
    "Dtmax = {'01': 100.27461126600004,\n",
    " '02': 111.27852548656281,\n",
    " '03': 188.8111269184575,\n",
    " '04': 218.25948507204254,\n",
    " '05': 140.40299960421459,\n",
    " '06': 114.23061990103015,\n",
    " '07': 145.60860570823075,\n",
    " '08': 122.52270975189279,\n",
    " '09': 137.45071610675376,\n",
    " '10': 114.12779094166119,\n",
    " '11': 139.3207171169968,\n",
    " '12': 211.48913707991554,\n",
    " '13': 198.34712908610084,\n",
    " '14': 134.23706918622923,\n",
    " '15': 88.35067937679354,\n",
    " '16': 148.98351384624613,\n",
    " '17': 145.07185770686453,\n",
    " '18': 150.0267067336913,\n",
    " '19': 252.46817648022636,\n",
    " '20': 121.88603567643541}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from _Library_HOSA import cluster_agg_max_distance, accept_cluster\n",
    "times_hos = pd.read_csv(\"/home/silvia/Desktop/ONSET_HOS/ONSET_DETECT_whole.csv\")     # onset deriving from STS\n",
    "times_hos[\"Accept\"] = \"FALSE\"\n",
    "\n",
    "BEST_key = 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.1' # Best setting in our studycase\n",
    "all_best_keys = ['stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.1',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.2',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.3',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.4',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_max']\n",
    "std_eq = 15 \n",
    "\n",
    "uu = []\n",
    "\n",
    "indi_concordi = list(times_hos[all_best_keys][(times_hos[all_best_keys].std(axis=1)<=std_eq) & (times_hos[BEST_key]>0)].index) # check different thresholds are in accordance\n",
    "times_concordi = times_hos[[\"trace_name\"]+all_best_keys].iloc[indi_concordi,:]\n",
    "\n",
    "ev_list = np.array([s[:12] for s in times_concordi[\"trace_name\"]])\n",
    "ev_uniq = list(set(ev_list))\n",
    "ev_uniq.sort()\n",
    "\n",
    "for ev in ev_uniq:\n",
    "    tmp = times_concordi[(ev_list==ev)]                         # select a single event\n",
    "    arr_list = np.array([s[16:18] for s in tmp[\"trace_name\"]])  # select a single array for each event (tipical arr_list=[\"01\", \"01\", \"10\"..] )\n",
    "    arr_uniq = list(set(arr_list))\n",
    "    arr_uniq.sort()\n",
    "    for arr in arr_uniq:\n",
    "        tmp_2 = tmp[(arr_list==arr)]                            # select a single array for each event          \n",
    "        picks = tmp_2[BEST_key]\n",
    "\n",
    "        if len(picks) <= 2:\n",
    "            # Reject\n",
    "            times_hos[\"Accept\"][picks.index] = \"FALSE\"\n",
    "\n",
    "\n",
    "        if len(picks) == 3:\n",
    "            # Check max - min\n",
    "            if (picks.max()-picks.min()) < Dtmax[arr]:\n",
    "                times_hos[\"Accept\"][picks.index] = \"TRUE\"\n",
    "\n",
    "        if len(picks) > 3:\n",
    "            picks = picks.sort_values()\n",
    "            picks_l = list(picks.values)\n",
    "            if len(picks) == 7:\n",
    "                uu.append(picks_l+[arr])\n",
    "            s,e = cluster_agg_max_distance(picks_l,dmax=Dtmax[arr])\n",
    "            # for i in range(len(s)):\n",
    "            #     print(picks_l[s[i]:e[i]+1])\n",
    "            indi = accept_cluster(s,e)\n",
    "\n",
    "            if indi >=0:\n",
    "                times_hos[\"Accept\"][picks[s[indi]:e[indi]+1].index] = \"TRUE\"\n",
    "\n",
    "                \n",
    "times_hos.to_csv(f\"/home/silvia/Desktop/ONSET_HOS/ONSET_DETECT_whole_checked(std{std_eq}_tolerance40)_New.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
