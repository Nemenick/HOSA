{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful: Libraries, Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import scipy.signal as sc_sig\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.integrate import simps\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "from obspy import UTCDateTime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasseDataset:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.centrato = False               # indicates whether the time window has been cut and centered (bool)\n",
    "        self.demeaned = False               # indicates whether the mean has been removed. Two types of means: will be string, \"noise\" or \"total\"\n",
    "        self.normalized = False             # indicates whether it is normalized. Two types: will be string, \"Max_track\" or \"Threshold_Num_samples_track\"\n",
    "        self.sismogramma = np.array([])     # np.array (,)\n",
    "        self.metadata = {}                  # dictionary of lists, not np.array\n",
    "\n",
    "    def leggi_custom_dataset(self, percorsohdf5, percorsocsv):\n",
    "        \"\"\"\n",
    "        Read ALL traces\n",
    "        to be saved only vertical component\n",
    "        \"\"\"\n",
    "        \n",
    "        filehdf5 = h5py.File(percorsohdf5, 'r')\n",
    "        self.sismogramma = filehdf5.get(\"dataset1\")\n",
    "        self.sismogramma = np.array(self.sismogramma)\n",
    "        self.metadata = 0\n",
    "        self.metadata = pd.read_csv(percorsocsv)\n",
    "\n",
    "        self.centrato = self.metadata[\"centrato\"][1]\n",
    "        self.demeaned = self.metadata[\"demeaned\"][1]\n",
    "        if \"normalized\" in self.metadata.keys():\n",
    "            self.normalized = self.metadata[\"normalized\"][1]\n",
    "        filehdf5.close()\n",
    "\n",
    "    def finestra(self, semiampiezza=0):\n",
    "        \"\"\"\n",
    "            cut and center the window\n",
    "            semiampiezza: number of samples (semiamplitude of the window)\n",
    "        \"\"\"\n",
    "        sismogramma = [0 for _ in range(len(self.sismogramma))]\n",
    "\n",
    "        if self.centrato:\n",
    "            if len(self.sismogramma[0]) > 2 * semiampiezza:\n",
    "                centro = len(self.sismogramma[0]) // 2\n",
    "                for i in range(len(self.sismogramma)):\n",
    "                    sismogramma[i] = self.sismogramma[i][centro - semiampiezza:\n",
    "                                                         centro + semiampiezza]\n",
    "                self.sismogramma = np.array(sismogramma)\n",
    "            else:\n",
    "                print(\"\\nAlredy centered with a smaller window\")\n",
    "                print(\"I do nothing\\n\")\n",
    "\n",
    "        else:\n",
    "            for i in range(len(self.sismogramma)):\n",
    "                if self.metadata[\"trace_P_arrival_sample\"][i] > semiampiezza:\n",
    "                    sismogramma[i] = self.sismogramma[i][int(self.metadata[\"trace_P_arrival_sample\"][i]) - semiampiezza:\n",
    "                                                         int(self.metadata[\"trace_P_arrival_sample\"][i]) + semiampiezza]\n",
    "                else:\n",
    "                    print(f\"short waveform! {i}\")\n",
    "                    stringa = \"#\"\n",
    "                    for _ in range(100):\n",
    "                        stringa = stringa + \"#\"\n",
    "                    warnings.warn(\"\\n\"+stringa+\"\\nWARNING: CHOSE A SMALLER WINDOW,\"\n",
    "                                               \"I DO NOTHING\\n\"+stringa)\n",
    "                    print(\"Semiamplitude = \", semiampiezza, \"P_arrival = \", self.metadata[\"trace_P_arrival_sample\"][i])\n",
    "                    input()\n",
    "        \n",
    "\n",
    "            self.sismogramma = np.array(sismogramma)\n",
    "\n",
    "            self.centrato = True\n",
    "\n",
    "    def demean(self, metodo: str = 'rumore', semiamp: int = 80):\n",
    "        \"\"\"\n",
    "            method \"totale\" -> mean computed on whole trace\n",
    "            method \"rumore\" -> mean computed on some samples before P arrival\n",
    "        \"\"\"\n",
    "        if metodo == \"totale\":\n",
    "            self.demeaned = \"totale\"\n",
    "            self.sismogramma = self.sismogramma - np.mean(self.sismogramma, axis=1).reshape(len(self.sismogramma),1)\n",
    "\n",
    "        if metodo == \"rumore\":\n",
    "            self.demeaned = \"rumore\"\n",
    "            if self.centrato:\n",
    "                lung = len(self.sismogramma[0])\n",
    "                self.sismogramma = self.sismogramma - np.mean(self.sismogramma[ : , lung//2-semiamp : lung//2-5], axis=1).reshape(len(self.sismogramma),1)\n",
    "\n",
    "            else:\n",
    "                for i in range(len(self.sismogramma)):\n",
    "                    start_5 = self.metadata[\"trace_P_arrival_sample\"][i] - 5\n",
    "                    self.sismogramma[i] = self.sismogramma[i] - \\\n",
    "                                          np.mean(self.sismogramma[i][start_5 - semiamp : start_5])\n",
    "\n",
    "        if metodo != \"rumore\" and metodo != \"totale\":\n",
    "            print(\"NOT ALLOWED METHOD\\n rumore or totale?\")\n",
    "            metodo = input()\n",
    "            if metodo == \"rumore\" or metodo == \"totale\":\n",
    "                self.demean(metodo)\n",
    "\n",
    "    def normalizza(self, soglia=20.):\n",
    "       \n",
    "        if soglia == \"None\" or soglia == None:\n",
    "            print(\"Normalize to the 'max'\")\n",
    "            \n",
    "            self.sismogramma = self.sismogramma * 1.0                \n",
    "            self.sismogramma = self.sismogramma / np.max([np.max(self.sismogramma,axis=1),-np.min(self.sismogramma,axis = 1)], axis = 0).reshape(len(self.sismogramma),1)\n",
    "\n",
    "            self.normalized = f\"Max_traccia_{len(self.sismogramma[0])}_di_samples\"\n",
    "        else:\n",
    "            lung_traccia = len(self.sismogramma[0])\n",
    "            self.sismogramma = self.sismogramma * 1.0             \n",
    "            sism_0_arr = self.sismogramma[:,0:lung_traccia//2-5]\n",
    "            self.sismogramma = self.sismogramma / (soglia * np.max([np.max(sism_0_arr,axis=1),-np.min(sism_0_arr,axis = 1)], axis = 0).reshape(len(sism_0_arr),1))\n",
    "            self.sismogramma[self.sismogramma > 1.0] = 1.0\n",
    "            self.sismogramma[self.sismogramma < -1.0] = -1.0\n",
    "            self.sismogramma = self.sismogramma / (np.max([np.max(self.sismogramma,axis=1),-np.min(self.sismogramma,axis = 1)], axis = 0).reshape(len(self.sismogramma),1))\n",
    "            self.normalized = f\"Soglia={soglia}_traccia_di_{len(self.sismogramma[0])}_samples\"\n",
    "            tmp = np.max([np.max(sism_0_arr,axis=1),-np.min(sism_0_arr,axis = 1)], axis = 0).reshape(len(sism_0_arr),1)\n",
    "            print(np.where(tmp==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for getting High Order Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freq_filter(signal,sf,freqs,type_filter=\"bandpass\", order_filter=2):\n",
    "    \"\"\" \n",
    "    sf: sampling rate of the input waveform\n",
    "    freqs: list of frequences (e.g. 2 for bandpass), or single float (e.g. for highpass)\n",
    "        sf sampling frequence  \"\"\"\n",
    "    # type_filter: ‘lowpass’, ‘highpass’, ‘bandpass’, ‘bandstop’\n",
    "\n",
    "    freqs=np.array(freqs)\n",
    "    filt_b1,filt_a1=sc_sig.butter(order_filter,freqs/(sf/2),btype=type_filter)\n",
    "    filtered_sig=sc_sig.filtfilt(filt_b1,filt_a1,sc_sig.detrend(signal))\n",
    "    return filtered_sig\n",
    "\n",
    "def sliding_window_view(arr, window_shape, steps):\n",
    "    # -*- coding: utf-8 -*-\n",
    "\n",
    "    \"\"\" Produce a view from a sliding, striding window over `arr`.\n",
    "        The window is only placed in 'valid' positions - no overlapping\n",
    "        over the boundary.\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : numpy.ndarray, shape=(...,[x, (...), z])\n",
    "            The array to slide the window over.\n",
    "        window_shape : Sequence[int]\n",
    "            The shape of the window to raster: [Wx, (...), Wz],\n",
    "            determines the length of [x, (...), z]\n",
    "        steps : Sequence[int]\n",
    "            The step size used when applying the window\n",
    "            along the [x, (...), z] directions: [Sx, (...), Sz]\n",
    "        Returns\n",
    "        -------\n",
    "        view of `arr`, shape=([X, (...), Z], ..., [Wx, (...), Wz])\n",
    "            Where X = (x - Wx) // Sx + 1\n",
    "        Notes\n",
    "        -----\n",
    "        In general, given\n",
    "          `out` = sliding_window_view(arr,\n",
    "                                      window_shape=[Wx, (...), Wz],\n",
    "                                      steps=[Sx, (...), Sz])\n",
    "           out[ix, (...), iz] = arr[..., ix*Sx:ix*Sx+Wx,  (...), iz*Sz:iz*Sz+Wz]\n",
    "         Examples\n",
    "         --------\n",
    "         >>> import numpy as np\n",
    "         >>> x = np.arange(9).reshape(3,3)\n",
    "         >>> x\n",
    "         array([[0, 1, 2],\n",
    "                [3, 4, 5],\n",
    "                [6, 7, 8]])\n",
    "         >>> y = sliding_window_view(x, window_shape=(2, 2), steps=(1, 1))\n",
    "         >>> y\n",
    "         array([[[[0, 1],\n",
    "                  [3, 4]],\n",
    "                 [[1, 2],\n",
    "                  [4, 5]]],\n",
    "                [[[3, 4],\n",
    "                  [6, 7]],\n",
    "                 [[4, 5],\n",
    "                  [7, 8]]]])\n",
    "        >>> np.shares_memory(x, y)\n",
    "         True\n",
    "        # Performing a neural net style 2D conv (correlation)\n",
    "        # placing a 4x4 filter with stride-1\n",
    "        >>> data = np.random.rand(10, 3, 16, 16)  # (N, C, H, W)\n",
    "        >>> filters = np.random.rand(5, 3, 4, 4)  # (F, C, Hf, Wf)\n",
    "        >>> windowed_data = sliding_window_view(data,\n",
    "        ...                                     window_shape=(4, 4),\n",
    "        ...                                     steps=(1, 1))\n",
    "        >>> conv_out = np.tensordot(filters,\n",
    "        ...                         windowed_data,\n",
    "        ...                         axes=[[1,2,3], [3,4,5]])\n",
    "        # (F, H', W', N) -> (N, F, H', W')\n",
    "        >>> conv_out = conv_out.transpose([3,0,1,2])\n",
    "         \"\"\"\n",
    "    import numpy as np\n",
    "    from numpy.lib.stride_tricks import as_strided\n",
    "    in_shape = np.array(arr.shape[-len(steps):])  # [x, (...), z]\n",
    "    window_shape = np.array(window_shape)  # [Wx, (...), Wz]\n",
    "    steps = np.array(steps)  # [Sx, (...), Sz]\n",
    "    nbytes = arr.strides[-1]  # size (bytes) of an element in `arr`\n",
    "\n",
    "    # number of per-byte steps to take to fill window\n",
    "    window_strides = tuple(np.cumprod(arr.shape[:0:-1])[::-1]) + (1,)\n",
    "    # number of per-byte steps to take to place window\n",
    "    step_strides = tuple(window_strides[-len(steps):] * steps)\n",
    "    # number of bytes to step to populate sliding window view\n",
    "    strides = tuple(int(i) * nbytes for i in step_strides + window_strides)\n",
    "\n",
    "    outshape = tuple((in_shape - window_shape) // steps + 1)\n",
    "    # outshape: ([X, (...), Z], ..., [Wx, (...), Wz])\n",
    "    outshape = outshape + arr.shape[:-len(steps)] + tuple(window_shape)\n",
    "    return as_strided(arr, shape=outshape, strides=strides, writeable=False)\n",
    "\n",
    "def get_hos(data, window_size, func):\n",
    "    \"\"\"\n",
    "    @param data: waveform\n",
    "    @param window_size: the moving window size of hos function\n",
    "    @param func: function of hos\n",
    "    @return: hos\n",
    "    \"\"\"\n",
    "\n",
    "    # detrend the waveform\n",
    "    data = scipy.signal.detrend(data)\n",
    "\n",
    "    # get a sliding window view of given np array\n",
    "    slid_view =  sliding_window_view(data, (window_size,), (1,))\n",
    "\n",
    "    # apply the function of slid_view along axis 1\n",
    "    return func(slid_view,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Higher Order Statistics\"\"\"\n",
    "def S_1(data, **kwargs):\n",
    "    return np.mean(data,**kwargs)\n",
    "\n",
    "def S_2(data,**kwargs):\n",
    "    return np.std(data,ddof=1,**kwargs)\n",
    "\n",
    "def S_3(data,**kwargs):\n",
    "    return scipy.stats.skew(data,**kwargs)\n",
    "\n",
    "def S_4(data,axis=1,**kwargs):\n",
    "    return scipy.stats.kurtosis(data,axis=axis,**kwargs)\n",
    "\n",
    "def S_6(data, axis=1):\n",
    "    # can be slower\n",
    "    return np.sum((data-np.mean(data,axis=axis)[:,None])**6,axis=axis)/(data.shape[1]-1)/np.std(data,ddof=1,axis=axis)**6-15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for getting the onsets and for condiotions of MCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_onset_4(waveform,window_size=100, threshold=[0.1], statistics=S_6, origin_sample=0, sampling_rate=200):\n",
    "    # Origin sample refers to \"event orignin time\". Constrain to be near the origin time!\n",
    "    # Number of threshold -> arbitrary\n",
    "\n",
    "    # get hos, here we use S_3, S_4, ...\n",
    "    hos = get_hos(waveform, window_size, statistics)\n",
    "    # smooth the S4\n",
    "    hos = np.convolve(hos, np.ones(3)/3, mode='valid')\n",
    "    # get first derivative\n",
    "    diff = np.diff(hos)\n",
    "    # narrow the search range to a region near the maximum\n",
    "    pre_window = 200 * sampling_rate//200\n",
    "\n",
    "\n",
    "    # Search near the max of HOS\n",
    "    lower_bound = np.argmax(hos) - pre_window\n",
    "    if lower_bound < 0:\n",
    "        lower_bound = 0\n",
    "    upper_bound = lower_bound + pre_window +  window_size\n",
    "\n",
    "    onsets = []\n",
    "    for i in range(len(threshold)):\n",
    "        try:\n",
    "            # find the onset larger than threshold[i] * maximum of diff\n",
    "            onsets.append(np.where(diff[lower_bound:upper_bound] > threshold[i] * np.max(diff))[0][0] + lower_bound + window_size)\n",
    "        except:\n",
    "            # use trigger position when nothing found\n",
    "            onsets.append(-100000)\n",
    "\n",
    "    try:\n",
    "        # find the onset corresponding to the maximum of diff\n",
    "        onset_max = np.argmax(diff[lower_bound:upper_bound]) + lower_bound + window_size\n",
    "    except:\n",
    "        onset_max = -100000\n",
    "    return onsets, diff, onset_max, lower_bound, hos\n",
    "\n",
    "def cluster_agg_max_distance(picks, dmax=300):\n",
    "    # agglomerative 1D hierarchical clustering with conditions on max intra-cluster distance\n",
    "\n",
    "    # starting fromm all single elements representing separate clusters,\n",
    "    # agglomerate Reciprocal Nearest Neighbour,\n",
    "    #     if diameter new cluster < dmax, accept new cluster\n",
    "    # I End the cycle when no new cluster is born\n",
    "\n",
    "    # picks have to be a sorted list!\n",
    "    pic_M = [[i] for i in picks]\n",
    "    Z = linkage(pic_M,\"complete\")       # \"compute\" the clustering procedure. returns the \"rappresentation of the dendrogram\"\n",
    "    crit = Z[:, 2]                      # distance between clusters at each step\n",
    "    flat_clusters = fcluster(Z, t=dmax, criterion='monocrit', monocrit=crit) # \"stops\" the clustering procedure based on criteria inside crit (distance<=dmax)\n",
    "\n",
    "    # sclust -> index of the starting point of each cluster (refered to the sorted list \"picks\")\n",
    "    # eclust -> index of the ending point of each cluster (refered to the sorted list \"picks\")\n",
    "    sclust=[0]\n",
    "    eclust=[]\n",
    "    for i in range(len(flat_clusters)):\n",
    "        if i !=0:\n",
    "            if flat_clusters[i-1] != flat_clusters[i]:\n",
    "                sclust.append(i)\n",
    "                eclust.append(i-1)\n",
    "    eclust.append(len(flat_clusters)-1)\n",
    "    return sclust, eclust   \n",
    "\n",
    "def accept_cluster(startclust:list,endclust:list):\n",
    "    # to be used in the case where two or more clusters are present!\n",
    "    \"\"\"\n",
    "    startclust[i]: index of starting point of i-th cluster\n",
    "    endtclust[i]: index of starting point of i-th cluster\n",
    "    e.g. i have the picks [1,2,3,50,51,100], grouped as: [[1,2,3], [50,51], 100]. We have:\n",
    "            startclust = [0,3,5]\n",
    "            endclust = [2,4,5]\n",
    "    \"\"\"\n",
    "    # Starting from startclust and endclust, the function returns the index of the \"accepted\" cluster.\n",
    "    # Denoting with p1 and p2 the numbers of onsets present in the two most populated clusters, P the number of total clusters,\n",
    "    # we accept the most popolous iif: \n",
    "        # 1) cluster maggiore comprende almeno metà dei pick (metà nel senso di //) o se ha 4 punti o di più\n",
    "        # 2) p1 >= 3 \n",
    "        # 3) p1 >= 2 * p2 \n",
    "\n",
    "    if len(startclust) != len(endclust):\n",
    "        raise Exception(\"Len startclust does not match endclust\")\n",
    "    \n",
    "    if len(startclust) == 1:       # Accept if only 1 cluster is provided\n",
    "        return 0\n",
    "    \n",
    "    index_ok = -1\n",
    "    size = np.array(endclust) - np.array(startclust) + 1 # diff contains the sizes of the clusters\n",
    "    ssort = np.sort(size)\n",
    "    if (ssort[-1] > (endclust[-1]+1)//2 ) and (ssort[-1] >= 3) and (ssort[-1] >= 2*ssort[-2]): \n",
    "        #Accept most popolous cluster!\n",
    "        index_ok = np.argmax(size)\n",
    "\n",
    "    return index_ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful to evaluate results (Semblance and SNR calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def semblance(u):\n",
    "    # https://doi.org/10.1093/gji/ggu311 eq. (4)\n",
    "    # u have to be:\n",
    "    #   1) each row aligned for its own arrival time\n",
    "    #   2) demeaned\n",
    "    u = np.array(u)\n",
    "    Num = np.sum(u, axis=0)**2\n",
    "    Den = np.sum(u*u, axis=0)\n",
    "    return simps(Num)/simps(Den)/len(u)\n",
    "\n",
    "def semblance_normalized_tracess(u):\n",
    "    # https://doi.org/10.1093/gji/ggu311 eq. (4)\n",
    "    # u have to be:\n",
    "    #   1) each row aligned for its own arrival time\n",
    "    #   2) demeaned\n",
    "    u = np.array(u)\n",
    "    # u = u - np.mean(u, axis=1).reshape(len(u),1) (not necessary if demeaned before (it is better) )\n",
    "    u = u / np.max([np.max(u,axis=1),-np.min(u,axis = 1)], axis = 0).reshape(len(u),1)\n",
    "    Num = np.sum(u, axis=0)**2\n",
    "    Den = np.sum(u*u, axis=0)\n",
    "    return simps(Num)/simps(Den)/len(u)\n",
    "\n",
    "def SNR2(Data, arrival, amp=4*200, source_sample=None, equalsize=False,freq=None):\n",
    "    # Calc the signal to noise ratio\n",
    "    if source_sample is None:\n",
    "        source_sample = Data.shape[1]//2\n",
    "    sig = []\n",
    "    noise = []\n",
    "    leng = Data.shape[1]\n",
    "    # select signal window      [arrival - shift ; arrival + amp -shift ]\n",
    "    # select noise window       [origin_time - amp; origin_time]\n",
    "    shift = max(amp//10,20)\n",
    "    for i in range(len(Data)):\n",
    "        arrivo = arrival[i]\n",
    "        if amp+1 < arrivo < leng-amp-1:\n",
    "            if freq is None: \n",
    "                sig.append(Data[i,arrivo-shift:arrivo+amp-shift])\n",
    "                noise.append(Data[i,source_sample-amp:source_sample])\n",
    "            else:\n",
    "                sig.append(freq_filter(Data[i,arrivo-shift:arrivo+amp-shift],200,freq,type_filter=\"highpass\"))\n",
    "                noise.append(freq_filter(Data[i,source_sample-amp:source_sample],200,freq,type_filter=\"highpass\"))\n",
    "        elif equalsize:             # to create fictious data, in order to insert snr in catalogue\n",
    "            sig.append([0 for _ in range(amp)])\n",
    "            noise.append([_ for _ in range(amp)])\n",
    "    sig = np.array(sig)\n",
    "    noise = np.array(noise)\n",
    "    res = np.std(sig,axis=1)/np.std(noise,axis=1)\n",
    "    print(res.shape)\n",
    "    return res\n",
    "\n",
    "def semblance_for_array(D,time, key, s_=50,s=50, ntraces=-1, normalize=True, filter_frequencies=None, semi_amp_filt = 500):\n",
    "    # return the values of semblance (in a list) for each event at fixed array.\n",
    "    \"\"\"\n",
    "    D:          dataset (complete)\n",
    "    time:       arrival times (pd.dataframe)\n",
    "    key:        key of the dataframe (to retrive info of arrivals)\n",
    "    ntraces:    if > 1, calc semblance only if number of traces for array == ntraces\n",
    "    \"\"\"\n",
    "    semblance_arr = []\n",
    "\n",
    "    event_list = np.array([s[:12] for s in time[\"trace_name\"]])\n",
    "    event_uniq = list(set(event_list))\n",
    "    event_uniq.sort()\n",
    "\n",
    "    for ev in event_uniq:\n",
    "        tmp = time[(event_list==ev)]                                # select a single event\n",
    "        arr_list = np.array([s[16:18] for s in tmp[\"trace_name\"]])  # select a single array for each event (tipical arr_list=[\"01\", \"01\", \"10\"..] )\n",
    "        arr_uniq = list(set(arr_list))\n",
    "        arr_uniq.sort()\n",
    "        for arr in arr_uniq:\n",
    "            tmp_2 = tmp[(arr_list==arr)]\n",
    "            if len(tmp_2) > 1:                                      # can't calculate semblance for 1 lonely trace\n",
    "                if len(tmp_2)==ntraces or ntraces==-1 or (ntraces==10 and len(tmp_2)==11):\n",
    "                    \n",
    "                    if filter_frequencies is None:\n",
    "                        u = [D.sismogramma[i][tmp_2[key][i]-s_:tmp_2[key][i]+s] - np.mean(D.sismogramma[i][tmp_2[key][i]-150:tmp_2[key][i]-10]) for j,i in enumerate(tmp_2.index)]\n",
    "                    else:\n",
    "                        try:\n",
    "                            u = [freq_filter(D.sismogramma[i][tmp_2[key][i]-semi_amp_filt:tmp_2[key][i]+semi_amp_filt],200,filter_frequencies,type_filter=\"bandpass\")[semi_amp_filt-s_:semi_amp_filt+s] for i in tmp_2.index]\n",
    "                        except Exception as e: \n",
    "                            print(f\"Error: {e},{tmp_2.index},{key}\")\n",
    "                            continue\n",
    "                        u=np.array(u)\n",
    "                        u=u-np.mean(u[:, 0:45], axis=1).reshape(len(u),1)\n",
    "                    cond=True\n",
    "                    for io in u:\n",
    "                        if len(io) != s_+s:\n",
    "                            cond = False\n",
    "\n",
    "                    if  cond:\n",
    "                        if normalize:\n",
    "                            semblance_arr.append(semblance_normalized_tracess(u)- 1/len(u))\n",
    "                        else:\n",
    "                            semblance_arr.append(semblance(u)- 1/len(u))\n",
    "                    else:\n",
    "                        print(f\"Length of traces is not ok!, {tmp_2.index},{key}\")\n",
    "\n",
    "    semblance_arr = np.array(semblance_arr)\n",
    "    return semblance_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Trace Stage (STS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library & read Data\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from obspy import UTCDateTime \n",
    "\n",
    "# paths of the dataset\n",
    "hd = \"Detect_all_data_extended.hdf5\"\n",
    "cs = \"Detect_all_metadata_extended.csv\"\n",
    "# Dataset containing the traces (D.seismogram) and other metadata (D.metadata)\n",
    "D = ClasseDataset()\n",
    "D.leggi_custom_dataset(hd,cs)                                       \n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Start STS\")\n",
    "\n",
    "# uu used to generate the output in a CSV file, save the onset for each trace \n",
    "uu = pd.DataFrame.from_dict(D.metadata[\"trace_name\"])              \n",
    "uu[\"trace_P_arrival_sample\"] = D.metadata[\"trace_P_arrival_sample\"]\n",
    "\n",
    "# p contains the various settings to be tested [statistic, filtertype, frequencies of filtering, window size of HOS, thersholds]\n",
    "p = [[S_4, \"bandpass\", [1,30], 300, [0.1,0.2,0.3,0.4]], \n",
    "     [S_6, \"bandpass\", [1,30], 300, [0.1,0.2,0.3,0.4]]]\n",
    "post_origin = 8\n",
    "\n",
    "# used to generate a string that report the used statistic\n",
    "names = [\"S_4\", \"S_n\"]                                                     \n",
    "indi = 0\n",
    "Tempo_Inizio = time.perf_counter()\n",
    "\n",
    "# cycle on settings\n",
    "for stat, filt, freq, wind, th in p:                                \n",
    "    for ii in range(10):\n",
    "         # free some memory, if needed\n",
    "        gc.collect()  \n",
    "\n",
    "    # key of the final dictionary\n",
    "    string_key = f\"stat: {str(names[indi])} type_filter: {filt} filter freq: {freq} window_width: {wind} tresh:\"    \n",
    "    print(\"I am working on impostation: \",hd,\"\\n\", string_key)\n",
    "    # I use different threshold for same setting\n",
    "    ons_th = [[] for i in range(len(th)) ]                             \n",
    "    ons_max = []\n",
    "    \n",
    "    for i in range(len(D.sismogramma)): \n",
    "        # register the sample corresponding to the event origin\n",
    "        or_s =  int((UTCDateTime(D.metadata[\"source_origin_time\"][i])- UTCDateTime(D.metadata[\"trace_start_time\"][i]))*D.metadata[\"sampling_rate\"][i])\n",
    "        try:\n",
    "            inizio = or_s-wind if or_s-wind > 0  else 0\n",
    "            fine = inizio + post_origin*int(D.metadata[\"sampling_rate\"][i])\n",
    "            # I extract the portion of the waveform from the arrival until post_origin seconds after\n",
    "            sig = freq_filter(D.sismogramma[i][inizio:fine], D.metadata[\"sampling_rate\"][i], freq, type_filter= filt)\n",
    "            # returns onsets for various th_s, CF (HOS derivative), onset corresponding to th=1, lower_bound, HOS\n",
    "            onset_th, diff, onset_max,lower_bound,hoss  = get_onset_4(sig, wind, threshold=th, statistics= stat) \n",
    "            for j in range(len(th)):\n",
    "                ons_th[j].append(onset_th[j] + or_s-wind)\n",
    "            ons_max.append(onset_max + or_s-wind)\n",
    "        except Exception as e: \n",
    "            print(e,\"\\nException occurred to trace\", i)\n",
    "            for j in range(len(th)):\n",
    "                ons_th[j].append(9*10**10)\n",
    "            ons_max.append(9*10**10)\n",
    "\n",
    "\n",
    "    for j in range(len(th)):\n",
    "        ons_th_tmp = np.array(ons_th[j])\n",
    "        uu = pd.concat([uu,pd.DataFrame.from_dict({f\"{string_key}_ons_th={th[j]}\":ons_th_tmp})],axis=1)\n",
    "\n",
    "    ons_max = np.array(ons_max)\n",
    "    uu = pd.concat([uu,pd.DataFrame.from_dict({f\"{string_key}_ons_max\":ons_max})],axis=1)\n",
    "    uu.to_csv(\"ONSET_DETECT_whole.csv\",index=False)\n",
    "    indi +=1\n",
    "print(\"\\nAlgoritmh ran for \", time.perf_counter()-Tempo_Inizio, \" to perform \", len(p), \"tests on \", len(D.sismogramma),\" traces each test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Channel Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum allowed distance for clustering for each array (dmaxs = )\n",
    "Dtmax = {'01': 100.27461126600004,\n",
    " '02': 111.27852548656281,\n",
    " '03': 188.8111269184575,\n",
    " '04': 218.25948507204254,\n",
    " '05': 140.40299960421459,\n",
    " '06': 114.23061990103015,\n",
    " '07': 145.60860570823075,\n",
    " '08': 122.52270975189279,\n",
    " '09': 137.45071610675376,\n",
    " '10': 114.12779094166119,\n",
    " '11': 139.3207171169968,\n",
    " '12': 211.48913707991554,\n",
    " '13': 198.34712908610084,\n",
    " '14': 134.23706918622923,\n",
    " '15': 88.35067937679354,\n",
    " '16': 148.98351384624613,\n",
    " '17': 145.07185770686453,\n",
    " '18': 150.0267067336913,\n",
    " '19': 252.46817648022636,\n",
    " '20': 121.88603567643541}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "times_hos = pd.read_csv(\"ONSET_DETECT_whole.csv\")     # onset deriving from STS\n",
    "times_hos[\"Accept\"] = \"FALSE\"\n",
    "\n",
    "BEST_key = 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.1' # Best setting in our studycase\n",
    "all_best_keys = ['stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.1',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.2',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.3',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_th=0.4',\n",
    "                 'stat: S_4 type_filter: bandpass filter freq: [1, 30] window_width: 300 tresh:_ons_max']\n",
    "std_eq = 15 \n",
    "\n",
    "uu = []\n",
    "\n",
    "indi_concordi = list(times_hos[all_best_keys][(times_hos[all_best_keys].std(axis=1)<=std_eq) & (times_hos[BEST_key]>0)].index) # check different thresholds are in accordance\n",
    "times_concordi = times_hos[[\"trace_name\"]+all_best_keys].iloc[indi_concordi,:]\n",
    "\n",
    "ev_list = np.array([s[:12] for s in times_concordi[\"trace_name\"]])\n",
    "ev_uniq = list(set(ev_list))\n",
    "ev_uniq.sort()\n",
    "\n",
    "for ev in ev_uniq:\n",
    "    tmp = times_concordi[(ev_list==ev)]                         # select a single event\n",
    "    arr_list = np.array([s[16:18] for s in tmp[\"trace_name\"]])  # select a single array for each event (tipical arr_list=[\"01\", \"01\", \"10\"..] )\n",
    "    arr_uniq = list(set(arr_list))\n",
    "    arr_uniq.sort()\n",
    "    for arr in arr_uniq:\n",
    "        tmp_2 = tmp[(arr_list==arr)]                            # select a single array for each event          \n",
    "        picks = tmp_2[BEST_key]\n",
    "\n",
    "        if len(picks) <= 2:\n",
    "            # Reject\n",
    "            times_hos[\"Accept\"][picks.index] = \"FALSE\"\n",
    "\n",
    "\n",
    "        if len(picks) == 3:\n",
    "            # Check max - min\n",
    "            if (picks.max()-picks.min()) < Dtmax[arr]:\n",
    "                times_hos[\"Accept\"][picks.index] = \"TRUE\"\n",
    "\n",
    "        if len(picks) > 3:\n",
    "            picks = picks.sort_values()\n",
    "            picks_l = list(picks.values)\n",
    "            if len(picks) == 7:\n",
    "                uu.append(picks_l+[arr])\n",
    "            s,e = cluster_agg_max_distance(picks_l,dmax=Dtmax[arr])\n",
    "            # for i in range(len(s)):\n",
    "            #     print(picks_l[s[i]:e[i]+1])\n",
    "            indi = accept_cluster(s,e)\n",
    "\n",
    "            if indi >=0:\n",
    "                times_hos[\"Accept\"][picks[s[indi]:e[indi]+1].index] = \"TRUE\"\n",
    "\n",
    "                \n",
    "times_hos.to_csv(f\"ONSET_DETECT_whole_checked(std{std_eq}_tolerance40)_New.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
